{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SSP_Yoonji_cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# import wandb\n",
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(description=\"SSP_Yoonji\")\n",
    "\n",
    "## DATA\n",
    "parser.add_argument(\"--valid_path\", default=\"/home/yoonji/ictc2024/dataset/val dataset\", type=str)\n",
    "parser.add_argument(\"--test_path\", default=\"/home/yoonji/ictc2024/dataset/test dataset\", type=str)\n",
    "parser.add_argument('--window_size', default=24, type=int)  # 수면 시간 고려하여 설정하였음\n",
    "parser.add_argument('--stride_size', default=1, type=int)  # 1시간 단위로 봄\n",
    "\n",
    "## MHA\n",
    "parser.add_argument('--num_head', default=8, type=int)\n",
    "parser.add_argument('--hid_dim', default=128, type=int)\n",
    "\n",
    "## TRAIN\n",
    "parser.add_argument('--optimizer', default=\"adamw\", type=str)\n",
    "parser.add_argument(\"--learning_rate\", default=1e-4, type=float)\n",
    "parser.add_argument(\"--weight_decay\", default=0, type=float)\n",
    "parser.add_argument('--scheduler', default=\"step\", type=str)\n",
    "parser.add_argument('--batch_size', default=16, type=int)\n",
    "parser.add_argument('--epochs', default=1000, type=int)\n",
    "parser.add_argument('--patience', default=100, type=int)\n",
    "\n",
    "parser.add_argument('--cv', default=5, type=int)\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--mixed_precision', default=32, type=int)\n",
    "parser.add_argument('--device', nargs='+', default=[0], type=int)\n",
    "parser.add_argument('--num_workers', default=0, type=int)\n",
    "\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# wandb.init(config=args, name='SSP_JY(GAG)', project=\"ETRI_Baseline\")\n",
    "# wandb_logger = WandbLogger(name='SSP_JY(GAG)', project=\"ETRI_Baseline\")\n",
    "# wandb.config.update(args)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "CFG = {\n",
    "    \"WINDOW_SIZE\" : args.window_size,\n",
    "    \"STRIDE_SIZE\" : args.stride_size,\n",
    "    \"BATCH_SIZE\" : args.batch_size,\n",
    "    \"EPOCHS\"     : args.epochs,\n",
    "    \"PATIENCE\"   : args.patience,\n",
    "    \"CV\"         : args.cv,\n",
    "    \"SEED\"       : args.seed,\n",
    "    \"VALID_PATH\" : args.valid_path,\n",
    "    \"TEST_PATH\"  : args.test_path,\n",
    "}\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    L.seed_everything(SEED)\n",
    "\n",
    "torch.set_float32_matmul_precision('high') \n",
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "idx = f\"{parser.description}_{device}\"\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv('/home/yoonji/ictc2024/dataset/val_label.csv')\n",
    "test_label  = pd.read_csv('/home/yoonji/ictc2024/dataset/answer_sample.csv')\n",
    "\n",
    "train_label['date'] = pd.to_datetime(train_label['date'])\n",
    "test_label['date']  = pd.to_datetime(test_label['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2520, 19), (2760, 19))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(os.path.join(CFG['VALID_PATH'],'train_data2.csv'))\n",
    "test_data  = pd.read_csv(os.path.join(CFG['TEST_PATH'], 'test_data2.csv'))\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {}\n",
    "\n",
    "for id in [1, 2, 3, 4]:\n",
    "    train_data_dict[f'train_macc_{id}'] = pd.read_csv(os.path.join(CFG['VALID_PATH'], f'train_macc_{id}-2.csv'))\n",
    "\n",
    "test_data_dict = {}\n",
    "\n",
    "for id in [5, 6, 7, 8]:\n",
    "    test_data_dict[f'test_macc_{id}'] = pd.read_csv(os.path.join(CFG['TEST_PATH'], f'test_macc_{id}-2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2520, 6), (2760, 6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = train_data_dict.keys()\n",
    "train_macc = pd.DataFrame()\n",
    "\n",
    "for key in keys:\n",
    "    train_macc = pd.concat([train_macc, train_data_dict[key]], axis=0)\n",
    "\n",
    "keys = test_data_dict.keys()\n",
    "test_macc = pd.DataFrame()\n",
    "\n",
    "for key in keys:\n",
    "    test_macc = pd.concat([test_macc, test_data_dict[key]], axis=0)\n",
    "\n",
    "train_macc.fillna(0, inplace=True)\n",
    "test_macc.fillna(0, inplace=True)\n",
    "\n",
    "train_macc.shape, test_macc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(train_macc, on=['subject_id', 'hour'], how='left')\n",
    "test_data  = test_data.merge(test_macc, on=['subject_id', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## categorical feature 처리\n",
    "\n",
    "def Info2Idx(df, cat_feat):\n",
    "    info2idx = {}\n",
    "    for f in cat_feat:\n",
    "        f_unique    = df[f].unique()\n",
    "        info2idx[f] = {k:v+1 for v, k in enumerate(f_unique)}\n",
    "    return info2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['hour'] = pd.to_datetime(train_data['hour'])\n",
    "test_data['hour']  = pd.to_datetime(test_data['hour'])\n",
    "\n",
    "train_data['time'] = train_data['hour'].dt.hour.astype(float)\n",
    "test_data['time']  = test_data['hour'].dt.hour.astype(float) # 시간 따로 뺌\n",
    "train_data['month'] = train_data['hour'].dt.month.astype(float)\n",
    "test_data['month']  = test_data['hour'].dt.month.astype(float) # 월 따로 뺌\n",
    "\n",
    "train_data['day'] = train_data['hour'].dt.dayofweek.astype(float)\n",
    "test_data['day']  = test_data['hour'].dt.dayofweek.astype(float) # 날짜 따로 뺌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = ['activity', 'month', 'max_ambience_cls']\n",
    "total_cat = pd.concat([train_data.loc[:, cat_feat], test_data.loc[:, cat_feat]], axis=0)\n",
    "\n",
    "info2idx = Info2Idx(total_cat, cat_feat) # 각 범주형 특징을 인덱스 변환\n",
    "\n",
    "\n",
    "train_data[cat_feat] = train_data[cat_feat].apply(lambda x: x.map(info2idx[x.name]))\n",
    "test_data[cat_feat]  = test_data[cat_feat].apply(lambda x: x.map(info2idx[x.name]))\n",
    "\n",
    "# args.f_sizes = [len(info2idx[i])+1 for i in cat_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(columns=['subject_id', 'hour'], inplace=True) \n",
    "train_data.drop(columns=['subject_id', 'hour'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 시퀀스 데이터 생성\n",
    "\n",
    "def create_sequences(data, labels, time_steps=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) // time_steps):\n",
    "        X.append(data[i*time_steps:(i+1)*time_steps])\n",
    "        y.append(labels[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "time_steps = 24\n",
    "X_train, y_train = create_sequences(train_data, train_label.iloc[:,2:].values, time_steps)\n",
    "X_test, y_test = create_sequences(test_data, test_label.iloc[:,2:].values, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False, worker_init_fn=seed_worker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim).to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_dim).to(device)\n",
    "        x, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.output(x[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Load model checkpoints\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 50\n",
    "output_dim = 1  # 각 평가지표별로 예측하기 때문에 output_dim은 1로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "models = []\n",
    "for i in range(1, 8):\n",
    "    model_path = f'/home/yoonji/ictc2024/trained_models/model_{i}_20240627_173253.pth'\n",
    "    model = LSTMModel(input_dim, hidden_dim, output_dim)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro for label 0: 0.4341\n",
      "F1 Macro for label 1: 0.4868\n",
      "F1 Macro for label 2: 0.5248\n",
      "F1 Macro for label 3: 0.4840\n",
      "F1 Macro for label 4: 0.3556\n",
      "F1 Macro for label 5: 0.4962\n",
      "F1 Macro for label 6: 0.5800\n",
      "Average F1 Macro over all labels: 0.4802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred = [[] for _ in range(y_test.shape[1])]\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        for j in range(y_test.shape[1]):\n",
    "            outputs = models[j](data)\n",
    "            y_pred[j].extend(outputs.cpu().numpy())\n",
    "\n",
    "y_pred = [np.array(pred) for pred in y_pred]\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# 각 평가지표별 F1 Macro Score 계산\n",
    "f1_macros = []\n",
    "for j in range(y_test.shape[1]):\n",
    "    f1_macro = f1_score(y_true[:, j], (y_pred[j] > 0.5).astype(int), average='macro')\n",
    "    f1_macros.append(f1_macro)\n",
    "    print(f'F1 Macro for label {j}: {f1_macro:.4f}')\n",
    "\n",
    "# 모든 평가지표에 대한 F1 Macro Score의 평균 계산\n",
    "average_f1_macro = np.mean(f1_macros)\n",
    "print(f'Average F1 Macro over all labels: {average_f1_macro:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = (np.array(y_pred)>0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "test_label.iloc[:, 2:] = final_pred\n",
    "test_label.to_csv(f'./dataset/submission_{idx}_{now.date()}_{now.time()}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
